{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence Assignment 1\n",
    "\n",
    "I provide here an Python implementation of the neural network described in the assignment.\n",
    "This code is an extension of the code provided in the lab week 2 of the module, therefore, a major part of the credit goes to this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import random\n",
    "import numpy.matlib \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd;\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples * (28*28) = (20800, 784)\n",
      "Training Labels (20800, 1)\n",
      "Testing Samples * (28*28) = (6500, 784)\n",
      "Testing Labels  (6500, 1)\n",
      "Validation Samples * (28*28) = (5200, 784)\n",
      "Validation Labels  (5200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the EMNIST dataset.\n",
    "# REF : Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). \n",
    "# EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n",
    "# Here the EMNIST data has been modified so the size is 1000 samples per class for training\n",
    "# and 250 per class for testing\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "emnist = loadmat('emnist-letters-1k.mat')\n",
    "\n",
    "# Read the train set\n",
    "x_train = emnist['train_images']\n",
    "# Read the train labels\n",
    "trainlabels = emnist['train_labels']\n",
    "\n",
    "# Read the test set\n",
    "x_test = emnist['test_images']\n",
    "# Read the test labels\n",
    "testlabels = emnist['test_labels']\n",
    "\n",
    "# Randomise Test set\n",
    "X_test, test_labels = shuffle(x_test, testlabels)\n",
    "\n",
    "# Apply the split between Train and Validation sets (Randomly selected from 20% of Training Set)\n",
    "X_train, X_validation, train_labels, validation_labels = train_test_split(\n",
    "    x_train, trainlabels, test_size = 0.2, random_state = None) \n",
    "\n",
    "# Normalise the Training Set\n",
    "NX_train = preprocessing.normalize(X_train)\n",
    "NX_test = preprocessing.normalize(X_test)\n",
    "NX_validation = preprocessing.normalize(X_validation)\n",
    "\n",
    "# Show the shape of each of these arrays\n",
    "# Our convention is for the first dimension to be the number of samples\n",
    "print(\"Training Samples * (28*28) =\", NX_train.shape)\n",
    "print(\"Training Labels\", train_labels.shape)\n",
    "\n",
    "print(\"Testing Samples * (28*28) =\", NX_test.shape)\n",
    "print(\"Testing Labels \", test_labels.shape)\n",
    "\n",
    "print(\"Validation Samples * (28*28) =\", NX_validation.shape)\n",
    "print(\"Validation Labels \", validation_labels.shape)\n",
    "n_samples, img_size = X_train.shape\n",
    "\n",
    "# The EMNIST contains letters from A to Z so we will set the number of labels as 26\n",
    "nlabels = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    }
   ],
   "source": [
    "# Example of image, take the transpose to see it in the right orientation\n",
    "example_image = np.reshape(NX_train[332], (28, 28))\n",
    "plt.imshow(example_image.T)\n",
    "plt.show()\n",
    "print(train_labels[332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   5  31  32   9   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32 183 200\n",
      " 138  63   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  37 217 249 246 125   4   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  37\n",
      " 217 250 250 127   4   0   0   0   0   0   0   0   0   2   4   4   4   0\n",
      "   0   0   0   0   0   0   0   0   0  37 217 250 250 127   4   0   0   0\n",
      "   0   0   0   0   1  20  37  37  32   7   0   0   0   0   0   0   0   0\n",
      "   0  37 215 249 250 127   4   0   0   0   0   0   0   0  22 154 215 217\n",
      " 201  77   2   0   0   0   0   0   0   0   0  21 172 233 250 127   4   0\n",
      "   0   0   0   0   0   7  95 232 254 254 249 125   4   0   0   0   0   0\n",
      "   0   0   0   5 127 216 250 141  11   1   0   0   0   7  20  64 207 253\n",
      " 254 254 232 109   3   0   0   0   0   0   0   0   0   2  81 170 251 221\n",
      "  91  36   5   9  34  95 159 221 249 254 247 231 127  22   0   0   0   0\n",
      "   0   0   0   0   0   0  46 126 244 245 164  96  39  51 115 177 220 250\n",
      " 254 253 221 175  67   4   0   0   0   0   0   0   0   0   0   0   7  33\n",
      " 163 243 245 233 217 222 245 252 254 254 253 219  91  36   3   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1  33 115 177 220 251 254 254 255\n",
      " 254 254 232  95   9   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   4  34  79 171 234 254 254 254 247 159  22   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3  31 159\n",
      " 251 254 253 207  47   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   3  79 220 254 252 234  95   7   0   0   0\n",
      "   0   4   4   2   0   0   0   0   0   0   0   0   0   0   0   0   0   8\n",
      " 126 244 254 248 218  51   1   0   0   0   8  32  32  21   2   0   0   0\n",
      "   0   0   0   0   0   0   0   0   4  34 204 254 252 188 127  19   0   0\n",
      "   3  20 122 203 203 169  65   2   0   0   0   0   0   0   0   0   0   0\n",
      "  34  95 234 254 245 116  36   0   3  21  52 100 231 253 248 229 107   3\n",
      "   0   0   0   0   0   0   0   0   0   4 113 202 253 254 209  46  11  11\n",
      "  79 170 220 234 252 250 207 131  22   0   0   0   0   0   0   0   0   0\n",
      "   0   4 114 203 254 251 159  59  83 139 220 251 254 253 221 139  77  32\n",
      "   0   0   0   0   0   0   0   0   0   0   0   2  81 170 251 252 192 143\n",
      " 172 220 247 254 250 243 163  52  20   7   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  22  79 218 253 252 250 252 254 253 221 139 115  33   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   8  90 202\n",
      " 217 217 217 217 202  91  11   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   7  32  37  37  37  37  32   7   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00170447\n",
      " 0.0105677  0.01090859 0.00306804 0.00068179 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01090859 0.06238352 0.06817871\n",
      " 0.04704331 0.02147629 0.00068179 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01261306 0.0739739  0.08488249 0.08385981 0.04261169\n",
      " 0.00136357 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01261306\n",
      " 0.0739739  0.08522339 0.08522339 0.04329348 0.00136357 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00068179 0.00136357 0.00136357 0.00136357 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01261306 0.0739739  0.08522339\n",
      " 0.08522339 0.04329348 0.00136357 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00034089 0.00681787\n",
      " 0.01261306 0.01261306 0.01090859 0.00238625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01261306 0.07329211 0.08488249 0.08522339 0.04329348\n",
      " 0.00136357 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00749966 0.05249761 0.07329211 0.0739739\n",
      " 0.0685196  0.0262488  0.00068179 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00715876\n",
      " 0.05863369 0.0794282  0.08522339 0.04329348 0.00136357 0.\n",
      " 0.         0.         0.         0.         0.         0.00238625\n",
      " 0.03238489 0.0790873  0.08658696 0.08658696 0.08488249 0.04261169\n",
      " 0.00136357 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00170447 0.04329348 0.07363301\n",
      " 0.08522339 0.04806599 0.00374983 0.00034089 0.         0.\n",
      " 0.         0.00238625 0.00681787 0.02181719 0.07056496 0.08624607\n",
      " 0.08658696 0.08658696 0.0790873  0.0371574  0.00102268 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00068179 0.02761238 0.0579519  0.08556428 0.07533747\n",
      " 0.03102131 0.01227217 0.00170447 0.00306804 0.01159038 0.03238489\n",
      " 0.05420207 0.07533747 0.08488249 0.08658696 0.08420071 0.07874641\n",
      " 0.04329348 0.00749966 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0156811  0.04295259 0.08317803 0.08351892 0.05590654 0.03272578\n",
      " 0.01329485 0.01738557 0.03920276 0.06033816 0.07499658 0.08522339\n",
      " 0.08658696 0.08624607 0.07533747 0.05965637 0.02283987 0.00136357\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00238625 0.01124949\n",
      " 0.05556565 0.08283713 0.08351892 0.0794282  0.0739739  0.07567837\n",
      " 0.08351892 0.08590517 0.08658696 0.08658696 0.08624607 0.07465569\n",
      " 0.03102131 0.01227217 0.00102268 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00034089 0.01124949 0.03920276\n",
      " 0.06033816 0.07499658 0.08556428 0.08658696 0.08658696 0.08692786\n",
      " 0.08658696 0.08658696 0.0790873  0.03238489 0.00306804 0.00034089\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00136357 0.01159038 0.02693059\n",
      " 0.0582928  0.07976909 0.08658696 0.08658696 0.08658696 0.08420071\n",
      " 0.05420207 0.00749966 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00102268 0.0105677  0.05420207\n",
      " 0.08556428 0.08658696 0.08624607 0.07056496 0.016022   0.00034089\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00102268 0.02693059 0.07499658 0.08658696 0.08590517\n",
      " 0.07976909 0.03238489 0.00238625 0.         0.         0.\n",
      " 0.         0.00136357 0.00136357 0.00068179 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00272715\n",
      " 0.04295259 0.08317803 0.08658696 0.0845416  0.07431479 0.01738557\n",
      " 0.00034089 0.         0.         0.         0.00272715 0.01090859\n",
      " 0.01090859 0.00715876 0.00068179 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00136357 0.01159038 0.06954228 0.08658696\n",
      " 0.08590517 0.06408799 0.04329348 0.00647698 0.         0.\n",
      " 0.00102268 0.00681787 0.04158901 0.06920139 0.06920139 0.05761101\n",
      " 0.02215808 0.00068179 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01159038 0.03238489 0.07976909 0.08658696 0.08351892 0.03954365\n",
      " 0.01227217 0.         0.00102268 0.00715876 0.01772646 0.03408935\n",
      " 0.07874641 0.08624607 0.0845416  0.07806462 0.03647561 0.00102268\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00136357 0.03852097 0.0688605\n",
      " 0.08624607 0.08658696 0.07124675 0.0156811  0.00374983 0.00374983\n",
      " 0.02693059 0.0579519  0.07499658 0.07976909 0.08590517 0.08522339\n",
      " 0.07056496 0.04465706 0.00749966 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00136357 0.03886186 0.06920139 0.08658696 0.08556428\n",
      " 0.05420207 0.02011272 0.02829416 0.0473842  0.07499658 0.08556428\n",
      " 0.08658696 0.08624607 0.07533747 0.0473842  0.0262488  0.01090859\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00068179\n",
      " 0.02761238 0.0579519  0.08556428 0.08590517 0.06545156 0.04874778\n",
      " 0.05863369 0.07499658 0.08420071 0.08658696 0.08522339 0.08283713\n",
      " 0.05556565 0.01772646 0.00681787 0.00238625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00749966 0.02693059\n",
      " 0.07431479 0.08624607 0.08590517 0.08522339 0.08590517 0.08658696\n",
      " 0.08624607 0.07533747 0.0473842  0.03920276 0.01124949 0.00034089\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00034089 0.00272715 0.03068042 0.0688605\n",
      " 0.0739739  0.0739739  0.0739739  0.0739739  0.0688605  0.03102131\n",
      " 0.00374983 0.00136357 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00238625 0.01090859 0.01261306 0.01261306\n",
      " 0.01261306 0.01261306 0.01090859 0.00238625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Just to see the effect of normalising\n",
    "print(X_train[112])\n",
    "print(NX_train[112])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Enconding\n",
    "\n",
    "y_train = np.zeros((train_labels.shape[0], nlabels))\n",
    "y_test  = np.zeros((test_labels.shape[0], nlabels))\n",
    "y_validation = np.zeros((validation_labels.shape[0], nlabels))\n",
    "\n",
    "for i in range(0,train_labels.shape[0]):   \n",
    "    y_train[i, train_labels[i].astype(int)]=1\n",
    "    \n",
    "for i in range(0,test_labels.shape[0]):    \n",
    "    y_test[i, test_labels[i].astype(int)]=1\n",
    "    \n",
    "for i in range(0,validation_labels.shape[0]):\n",
    "    y_validation[i, validation_labels[i].astype(int)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "n_epoch = 75 # 100 ?\n",
    "batch_size = 50\n",
    "n_batches = int(math.ceil(n_samples/batch_size))\n",
    "\n",
    "# define the size of each of the layers in the network\n",
    "n_input_layer  = img_size\n",
    "n_hidden_layer = 100\n",
    "n_output_layer = nlabels\n",
    "\n",
    "# Add another hidden layer\n",
    "n_hidden_layer2 = 200 # number of neurons of the hidden layer. 0 deletes this layer\n",
    "\n",
    "# eta is the learning rate\n",
    "eta = 0.05\n",
    "# Lambda for l1 penalty\n",
    "lam = 0.00007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ReLU Function\n",
    "def ReLU(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Gradient ReLU Function\n",
    "def Der_ReLU(x):\n",
    "    return np.greater(x, 0).astype(int)\n",
    "\n",
    "# Plot ReLU Function\n",
    "x = [x for x in range(-10, 11)]\n",
    "y = [ReLU(x) for x in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU')\n",
    "plt.title('ReLU as a function of x')\n",
    "plt.show()\n",
    "\n",
    "# The derivative of ReLU is :\n",
    "# ReLU'(x) = 0 if x < 0\n",
    "# ReLU'(x) = 1 if x > 0\n",
    "# Not defined if x = 0\n",
    "\n",
    "# Gradient ReLU Function\n",
    "def Der_ReLU(x):\n",
    "    return np.greater(x, 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization\n",
    "\n",
    "He et. a has proposed an alternative to the Xavier Initialization to make it more fit to the use of the ReLU activation function. Below, you can find the paper in which they are introducing it.\n",
    "\n",
    "https://arxiv.org/pdf/1502.01852.pdf\n",
    "\n",
    "If you want more information, below is the article that made me opt out for the He initilization method for this assignment.\n",
    "\n",
    "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(500, 100)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a simple network\n",
    "# For W1 and W2 columns are the input and the rows are the output.\n",
    "# W1: Number of columns (input) needs to be equal to the number of features \n",
    "#     of the  EMNIST letters, thus p. Number of rows (output) should be equal \n",
    "#     to the number of neurons of the hidden layer thus n_hidden_layer.\n",
    "# W2: Number of columns (input) needs to be equal to the number of neurons \n",
    "#     of the hidden layer. Number of rows (output) should be equal to the \n",
    "#     number of digits we wish to find (classification).\n",
    "\n",
    "Xavier_init=False\n",
    "He_init=True\n",
    "\n",
    "if Xavier_init:\n",
    "    W1 = np.random.randn(n_hidden_layer, n_input_layer) * np.sqrt(1 / (n_input_layer))\n",
    "    if n_hidden_layer2>0:\n",
    "        W2 = np.random.randn(n_hidden_layer2, n_hidden_layer) * np.sqrt(1 / (n_hidden_layer))\n",
    "        W3 = np.random.randn(n_output_layer, n_hidden_layer2) * np.sqrt(1 / (n_hidden_layer2))\n",
    "    else:\n",
    "        W2 = np.random.randn(n_output_layer, n_hidden_layer) * np.sqrt(1 / (n_hidden_layer))\n",
    "        \n",
    "elif He_init:\n",
    "    W1 = np.random.randn(n_hidden_layer, n_input_layer) * np.sqrt(2 / (n_input_layer))\n",
    "    if n_hidden_layer2>0:\n",
    "        W2 = np.random.randn(n_hidden_layer2, n_hidden_layer) * np.sqrt(2 / (n_hidden_layer))\n",
    "        W3 = np.random.randn(n_output_layer, n_hidden_layer2) * np.sqrt(2 / (n_hidden_layer2))\n",
    "    else:\n",
    "        W2 = np.random.randn(n_output_layer, n_hidden_layer) * np.sqrt(2 / (n_hidden_layer))\n",
    "        \n",
    "else:\n",
    "    W1 = np.random.uniform(0,1,(n_hidden_layer, n_input_layer))\n",
    "    W2 = np.random.uniform(0,1,(n_output_layer, n_hidden_layer))\n",
    "\n",
    "    # The following normalises the random weights so that the sum of each row =1\n",
    "    W1 = np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,n_input_layer))\n",
    "    W2 = np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer))\n",
    "\n",
    "    if n_hidden_layer2>0:\n",
    "        W3=np.random.uniform(0,1,(n_output_layer,n_hidden_layer2))\n",
    "        W3=np.divide(W3,np.matlib.repmat(np.sum(W3,1)[:,None],1,n_hidden_layer2))\n",
    "\n",
    "        W2=np.random.uniform(0,1,(n_hidden_layer2,n_hidden_layer))\n",
    "        W2=np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer))\n",
    "        \n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the biases\n",
    "bias_W1 = np.zeros((n_hidden_layer,))\n",
    "bias_W2 = np.zeros((n_output_layer,))\n",
    "\n",
    "if n_hidden_layer2>0:    \n",
    "    bias_W3=np.zeros((n_output_layer,))\n",
    "    bias_W2=np.zeros((n_hidden_layer2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the network inputs and average error per epoch\n",
    "errors = np.zeros((n_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise, if element > 0 -> 1, else -> -1\n",
    "# Later on multiply by the scalar : lambda\n",
    "def der_l1_pen(a):\n",
    "    a = np.where(a > 0, 1, a)\n",
    "    a = np.where(a <= 0, -1, a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : error =  0.29662792901802365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92645914986b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m#d2_l1 = lam * der_l1_pen(dW1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mdW1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#- d2_l1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mdbias_W1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mouter\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the network\n",
    "\n",
    "for i in range(0, n_epoch):\n",
    "    \n",
    "    # Initialise the gradients for each batch\n",
    "    dW1 = np.zeros(W1.shape)\n",
    "    dW2 = np.zeros(W2.shape)\n",
    "    \n",
    "    # We will shuffle the order of the samples each epoch\n",
    "    shuffled_idxs = np.random.permutation(n_samples)\n",
    "    \n",
    "    for batch in range(0,n_batches):\n",
    "        # Initialise the gradients for each batch\n",
    "        dW1 = np.zeros(W1.shape)\n",
    "        dW2 = np.zeros(W2.shape)\n",
    "\n",
    "        dbias_W1 = np.zeros(bias_W1.shape)\n",
    "        dbias_W2 = np.zeros(bias_W2.shape)\n",
    "        \n",
    "        if n_hidden_layer2 > 0:\n",
    "            dW3 = np.zeros(W3.shape)\n",
    "            dbias_W3 = np.zeros(bias_W3.shape)\n",
    "            \n",
    "        # Loop over all the samples in the batch\n",
    "        for j in range(0,batch_size):\n",
    "\n",
    "            # Input (random element from the dataset)\n",
    "            idx = shuffled_idxs[batch*batch_size + j]\n",
    "            x0 = NX_train[idx]\n",
    "            \n",
    "            # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "            desired_output = y_train[idx]\n",
    "\n",
    "            # Neural activation: input layer -> hidden layer\n",
    "            h1 = np.dot(W1,x0)+bias_W1\n",
    "\n",
    "            # Apply the ReLU function\n",
    "            x1 = np.maximum(0, h1)\n",
    "\n",
    "            # Neural activation: hidden layer -> output layer\n",
    "            h2 = np.dot(W2,x1)+bias_W2\n",
    "\n",
    "            # Apply the ReLU function\n",
    "            x2 = np.maximum(0, h2)\n",
    "            \n",
    "            if n_hidden_layer2 > 0:\n",
    "                # Neural activation: hidden layer 1 -> hidden layer 2\n",
    "                h3 = np.dot(W3,x2)+bias_W3\n",
    "\n",
    "                # Apply the sigmoid function\n",
    "                x3 = np.maximum(0, h3)\n",
    "                \n",
    "                # Compute the error signal\n",
    "                e_n = desired_output - x3\n",
    "                \n",
    "                # Backpropagation: output layer -> hidden layer 2\n",
    "                delta3 = Der_ReLU(x3) * e_n\n",
    "                \n",
    "                dW3 += np.outer(delta3,x2)\n",
    "                dbias_W3 += delta3\n",
    "                \n",
    "                # Backpropagation: hidden layer -> input layer\n",
    "                delta2 = Der_ReLU(x2) * np.dot(W3.T, delta3) \n",
    "                \n",
    "            else:\n",
    "                # Compute the error signal\n",
    "                e_n = desired_output - x2\n",
    "                \n",
    "                # Backpropagation: output layer -> hidden layer\n",
    "                delta2 = Der_ReLU(x2) * e_n\n",
    "            \n",
    "            # Compute the L1 penalty error for output -> hidden\n",
    "            #s_l1 = lam * np.sum(np.absolute(dW2))\n",
    "            #d_l1 = lam * der_l1_pen(dW2)\n",
    "            \n",
    "            dW2 += np.outer(delta2, x1) #- d_l1\n",
    "            dbias_W2 += delta2\n",
    "\n",
    "            # Backpropagation: hidden layer -> input layer\n",
    "            delta1 = Der_ReLU(x1) * np.dot(W2.T, delta2)\n",
    "            \n",
    "            # Compute the L1 penalty error  for hidden -> input\n",
    "            #s2_l1 = lam * np.sum(np.absolute(dW1))\n",
    "            #d2_l1 = lam * der_l1_pen(dW1)\n",
    "            \n",
    "            dW1 += np.outer(delta1,x0) #- d2_l1\n",
    "            dbias_W1 += delta1\n",
    "\n",
    "            # Store the error per epoch\n",
    "            errors[i] = errors[i] + 0.5 * np.sum(np.square(e_n))/n_samples\n",
    "        \n",
    "        # Calculate the Penalty\n",
    "        d_l1 = lam * der_l1_pen(dW2)\n",
    "        d2_l1 = lam * der_l1_pen(dW1)\n",
    "        d3_l1 = lam * der_l1_pen(dW3)\n",
    "        # After each batch update the weights using accumulated gradients, apply penalty\n",
    "        W2 += eta*dW2 - d_l1 /batch_size\n",
    "        W1 += eta*dW1 - d2_l1 /batch_size\n",
    "\n",
    "        bias_W1 += eta*dbias_W1/batch_size\n",
    "        bias_W2 += eta*dbias_W2/batch_size\n",
    "        \n",
    "        if n_hidden_layer2 > 0:\n",
    "            W3 += eta*dW3 - d3_l1/batch_size\n",
    "            bias_W3 += eta*dbias_W3/batch_size\n",
    "    print( \"Epoch \", i+1, \": error = \", errors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Average error per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the test set to compute the network's accuracy\n",
    "n = NX_test.shape[0]\n",
    "p_ra = 0\n",
    "correct_value = np.zeros((n,))\n",
    "predicted_value = np.zeros((n,))\n",
    "\n",
    "for i in range(0, n):\n",
    "    x0 = NX_test[i]\n",
    "    y = y_test[i] \n",
    "    \n",
    "    correct_value[i] = np.argmax(y)\n",
    "    \n",
    "    h1 = np.dot(W1, x0) + bias_W1\n",
    "    x1 = np.maximum(0, h1)\n",
    "    \n",
    "    h2 = np.dot(W2, x1) + bias_W2\n",
    "    x2 = np.maximum(0, h2)\n",
    "    \n",
    "    if n_hidden_layer2 > 0:     \n",
    "        h3 = np.dot(W3, x2) + bias_W3    \n",
    "        x3 = np.maximum(0, h3)\n",
    "        \n",
    "        predicted_value[i] = np.argmax(x3)\n",
    "    \n",
    "    else:            \n",
    "        predicted_value[i] = np.argmax(x2)\n",
    "            \n",
    "    if predicted_value[i] == correct_value[i]: \n",
    "        p_ra = (p_ra + 1)\n",
    "\n",
    "accuracy = 100*p_ra/(n)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the test set to compute the network's accuracy\n",
    "n = NX_validation.shape[0]\n",
    "p_ra = 0\n",
    "correct_value = np.zeros((n,))\n",
    "predicted_value = np.zeros((n,))\n",
    "\n",
    "for i in range(0, n):\n",
    "    x0 = NX_validation[i]\n",
    "    y = y_validation[i] \n",
    "    \n",
    "    correct_value[i] = np.argmax(y)\n",
    "    \n",
    "    h1 = np.dot(W1, x0) + bias_W1\n",
    "    x1 = np.maximum(0, h1)\n",
    "    \n",
    "    h2 = np.dot(W2, x1) + bias_W2\n",
    "    x2 = np.maximum(0, h2)\n",
    "    \n",
    "    if n_hidden_layer2 > 0:     \n",
    "        h3 = np.dot(W3, x2) + bias_W3    \n",
    "        x3 = np.maximum(0, h3)\n",
    "        \n",
    "        predicted_value[i] = np.argmax(x3)\n",
    "    \n",
    "    else:            \n",
    "        predicted_value[i] = np.argmax(x2)\n",
    "            \n",
    "    if predicted_value[i] == correct_value[i]: \n",
    "        p_ra = (p_ra + 1)\n",
    "\n",
    "accuracy = 100*p_ra/(n)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
